{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import the required libraries and load the dataset into our application. The following script imports the required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From\n",
    "https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the dataset into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"../Dataset/cleanedReviewsDateset100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script displays the shape of the dataset and it also prints the header of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set.shape)\n",
    "\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the comment count for each label. To do so, we will first filter all the label or output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_categories_labels = data_set[[\"invalidPositionOverTime\", \"implementationResponseIssue\", \"invalidContextOverTime\", \"interruptedEvent\", \n",
    "                        \"invalidEventOccurraceOverTime\",\"actionNotPossible\",\"actionWhenNotAllowed\",\"informationOutOfOrder\",\"lackOfRequiredInformation\",\n",
    "                        \"invalidInfoAccess\",\"objectOutOfBoundForAnyState\",\"objectOutOfBoundForSpecificState\",\"artificialStupidity\",\"invalidValueChange\",\n",
    "                        \"invalidGraphicalRespresentation\",\"haveBugs\"]]\n",
    "bug_categories_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will plot bar plots that show the total comment counts for different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "bug_categories_labels.sum(axis=0).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Multi-label Text Classification Models:\n",
    "There are two ways to create multi-label classification models: Using single dense output layer and using multiple dense output layers.\n",
    "\n",
    "In the first approach, we can use a single dense layer with six outputs with a sigmoid activation functions and binary cross entropy loss functions. Each neuron in the output dense layer will represent one of the six output labels. The sigmoid activation function will return a value between 0 and 1 for each neuron. If any neuron's output value is greater than 0.5, it is assumed that the comment belongs to the class represented by that particular neuron.\n",
    "\n",
    "In the second approach we will create one dense output layer for each label. We will have a total of 6 dense layers in the output. Each layer will have its own sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-lable Text Classification Model with Single Output Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will create our input and output set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(data_set[\"text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "y = data_set[[\"invalidPositionOverTime\", \"implementationResponseIssue\", \"invalidContextOverTime\", \"interruptedEvent\", \n",
    "                        \"invalidEventOccurraceOverTime\",\"actionNotPossible\",\"actionWhenNotAllowed\",\"informationOutOfOrder\",\"lackOfRequiredInformation\",\n",
    "                        \"invalidInfoAccess\",\"objectOutOfBoundForAnyState\",\"objectOutOfBoundForSpecificState\",\"artificialStupidity\",\"invalidValueChange\",\n",
    "                        \"invalidGraphicalRespresentation\",\"haveBugs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do not need to perform any one-hot encoding because our output labels are already in the form of one-hot encoded vectors.\n",
    "\n",
    "In the next step, we will divide our data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=79,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y1_train = y_train[[\"invalidPositionOverTime\"]].values\n",
    "y1_test =  y_test[[\"invalidPositionOverTime\"]].values\n",
    "\n",
    "y2_train = y_train[[\"implementationResponseIssue\"]].values\n",
    "y2_test =  y_test[[\"implementationResponseIssue\"]].values\n",
    "\n",
    "y3_train = y_train[[\"invalidContextOverTime\"]].values\n",
    "y3_test =  y_test[[\"invalidContextOverTime\"]].values\n",
    "\n",
    "y4_train = y_train[[\"interruptedEvent\"]].values\n",
    "y4_test =  y_test[[\"interruptedEvent\"]].values\n",
    "\n",
    "y5_train = y_train[[\"invalidEventOccurraceOverTime\"]].values\n",
    "y5_test =  y_test[[\"invalidEventOccurraceOverTime\"]].values\n",
    "\n",
    "y6_train = y_train[[\"actionNotPossible\"]].values\n",
    "y6_test =  y_test[[\"actionNotPossible\"]].values\n",
    "\n",
    "y7_train = y_train[[\"actionWhenNotAllowed\"]].values\n",
    "y7_test =  y_test[[\"actionWhenNotAllowed\"]].values\n",
    "\n",
    "y8_train = y_train[[\"informationOutOfOrder\"]].values\n",
    "y8_test =  y_test[[\"informationOutOfOrder\"]].values\n",
    "\n",
    "y9_train = y_train[[\"lackOfRequiredInformation\"]].values\n",
    "y9_test =  y_test[[\"lackOfRequiredInformation\"]].values\n",
    "\n",
    "y10_train = y_train[[\"invalidInfoAccess\"]].values\n",
    "y10_test =  y_test[[\"invalidInfoAccess\"]].values\n",
    "\n",
    "y11_train = y_train[[\"objectOutOfBoundForAnyState\"]].values\n",
    "y11_test =  y_test[[\"objectOutOfBoundForAnyState\"]].values\n",
    "\n",
    "y12_train = y_train[[\"objectOutOfBoundForSpecificState\"]].values\n",
    "y12_test =  y_test[[\"objectOutOfBoundForSpecificState\"]].values\n",
    "\n",
    "y13_train = y_train[[\"artificialStupidity\"]].values\n",
    "y13_test =  y_test[[\"artificialStupidity\"]].values\n",
    "\n",
    "y14_train = y_train[[\"invalidValueChange\"]].values\n",
    "y14_test =  y_test[[\"invalidValueChange\"]].values\n",
    "\n",
    "y15_train = y_train[[\"invalidGraphicalRespresentation\"]].values\n",
    "y15_test =  y_test[[\"invalidGraphicalRespresentation\"]].values\n",
    "\n",
    "y16_train = y_train[[\"haveBugs\"]].values\n",
    "y16_test =  y_test[[\"haveBugs\"]].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert text inputs into embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using GloVe word embeddings to convert text inputs to their numeric counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('../kaggle/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script creates the model. Our model will have one input layer, one embedding layer, one LSTM layer with 128 neurons and one output layer with 6 neurons since we have 6 labels in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
    "LSTM_Layer1 = LSTM(128)(embedding_layer)\n",
    "\n",
    "output1 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output2 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output3 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output4 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output5 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output6 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output7 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output8 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output9 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output10 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output11 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output12 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output13 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output14 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output15 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "output16 = Dense(1, activation='sigmoid')(LSTM_Layer1)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9,output10,output11,output12,output13,output14,output15,output16])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script prints the architecture of our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=[y1_train, y2_train, y3_train, y4_train, y5_train, y6_train,y7_train,y8_train\n",
    ",y9_train,y10_train,y11_train,y12_train,y13_train,y14_train,y15_train,y16_train], batch_size=8192, epochs=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=X_test, y=[y1_test, y2_test, y3_test, y4_test, y5_test, y6_test, y7_test, y8_test, y9_test, y10_test, y11_test, y12_test, y13_test, y14_test, y15_test,y16_test], verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the loss and accuracy values for training and test sets to see if our model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['dense_acc'])\n",
    "plt.plot(history.history['val_dense_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['dense_loss'])\n",
    "plt.plot(history.history['val_dense_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29040de7e6f5de5d2bd1b40f6cd95c54deb9d87fe13eb19a8903168d595b640b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
