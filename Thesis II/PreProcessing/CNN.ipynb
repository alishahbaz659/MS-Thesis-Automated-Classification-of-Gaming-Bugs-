{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Convolutional Neural Network (CNN) with Word2Vec embeddings as input to detect bugs in gaming Reviews in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections.abc import Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the original dataset:\n",
      "\n",
      "Index(['product_id', 'page', 'page_order', 'recommended', 'date', 'bugType',\n",
      "       'haveBugs', 'text', 'hours', 'username', 'products', 'early_access',\n",
      "       'products_ismissing'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "top_data_df = pd.read_csv(\"Dataset/cleanedReviewsDateset100.csv\")\n",
    "print(\"Columns in the original dataset:\\n\")\n",
    "print(top_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After segregating and taking equal number of rows for each review:\n",
      "0    60\n",
      "1    39\n",
      "Name: haveBugs, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>page</th>\n",
       "      <th>page_order</th>\n",
       "      <th>recommended</th>\n",
       "      <th>date</th>\n",
       "      <th>bugType</th>\n",
       "      <th>haveBugs</th>\n",
       "      <th>text</th>\n",
       "      <th>hours</th>\n",
       "      <th>username</th>\n",
       "      <th>products</th>\n",
       "      <th>early_access</th>\n",
       "      <th>products_ismissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108600</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>invalid event occurance over time</td>\n",
       "      <td>1</td>\n",
       "      <td>fun game ruined by overly punishing update mor...</td>\n",
       "      <td>180.9</td>\n",
       "      <td>kobogen</td>\n",
       "      <td>157</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1086940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>implementation response issue</td>\n",
       "      <td>1</td>\n",
       "      <td>like its definitely fun but the stability is a...</td>\n",
       "      <td>81.4</td>\n",
       "      <td>wickedmystic</td>\n",
       "      <td>142</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1086940</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>information out of order</td>\n",
       "      <td>1</td>\n",
       "      <td>so many systems to learn its a bit overwhelmin...</td>\n",
       "      <td>39.2</td>\n",
       "      <td>voxix</td>\n",
       "      <td>120</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1086940</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>invalid value change</td>\n",
       "      <td>1</td>\n",
       "      <td>this game needs a higher level cap not worth p...</td>\n",
       "      <td>138.3</td>\n",
       "      <td>zardu hasselfrau</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1086940</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>invalid graphical representation, artificial s...</td>\n",
       "      <td>1</td>\n",
       "      <td>the game has film grain that takes away the sp...</td>\n",
       "      <td>33.2</td>\n",
       "      <td>tatical</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1826960</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>invalid event occurance over time</td>\n",
       "      <td>1</td>\n",
       "      <td>the game itself isnt bad its unique in the way...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>elisabeth</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1826960</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>invalid context state over time</td>\n",
       "      <td>1</td>\n",
       "      <td>i have to put this game down because all the h...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>urimiya</td>\n",
       "      <td>114</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1826960</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>artificial stupidity</td>\n",
       "      <td>1</td>\n",
       "      <td>so before us is the classic browser strategy o...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>n1k0msl</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>887570</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>lack of required information, information our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>this game is cool but it is waaaaaay too niche...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>918informant</td>\n",
       "      <td>187</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>887570</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>01-01-22</td>\n",
       "      <td>implementation response issue</td>\n",
       "      <td>1</td>\n",
       "      <td>it doesnt make sense that this game has overwh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>sentrysteve</td>\n",
       "      <td>188</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id  page  page_order  recommended      date  \\\n",
       "2       108600     1           7        False  01-01-22   \n",
       "5      1086940     1           1        False  01-01-22   \n",
       "6      1086940     1           2         True  01-01-22   \n",
       "8      1086940     1           6        False  01-01-22   \n",
       "10     1086940     1           8         True  01-01-22   \n",
       "17     1826960     1           2         True  01-01-22   \n",
       "18     1826960     1           3        False  01-01-22   \n",
       "20     1826960     1           7         True  01-01-22   \n",
       "27      887570     1           7         True  01-01-22   \n",
       "28      887570     1           8        False  01-01-22   \n",
       "\n",
       "                                              bugType  haveBugs  \\\n",
       "2                   invalid event occurance over time         1   \n",
       "5                       implementation response issue         1   \n",
       "6                            information out of order         1   \n",
       "8                                invalid value change         1   \n",
       "10  invalid graphical representation, artificial s...         1   \n",
       "17                  invalid event occurance over time         1   \n",
       "18                    invalid context state over time         1   \n",
       "20                               artificial stupidity         1   \n",
       "27  lack of required information, information our ...         1   \n",
       "28                      implementation response issue         1   \n",
       "\n",
       "                                                 text  hours  \\\n",
       "2   fun game ruined by overly punishing update mor...  180.9   \n",
       "5   like its definitely fun but the stability is a...   81.4   \n",
       "6   so many systems to learn its a bit overwhelmin...   39.2   \n",
       "8   this game needs a higher level cap not worth p...  138.3   \n",
       "10  the game has film grain that takes away the sp...   33.2   \n",
       "17  the game itself isnt bad its unique in the way...    0.9   \n",
       "18  i have to put this game down because all the h...    0.8   \n",
       "20  so before us is the classic browser strategy o...    0.8   \n",
       "27  this game is cool but it is waaaaaay too niche...    4.0   \n",
       "28  it doesnt make sense that this game has overwh...    5.0   \n",
       "\n",
       "            username  products  early_access  products_ismissing  \n",
       "2            kobogen       157          True               False  \n",
       "5       wickedmystic       142          True               False  \n",
       "6              voxix       120          True               False  \n",
       "8   zardu hasselfrau        37          True               False  \n",
       "10           tatical        47          True               False  \n",
       "17         elisabeth        12          True               False  \n",
       "18           urimiya       114          True               False  \n",
       "20           n1k0msl        26          True               False  \n",
       "27      918informant       187          True               False  \n",
       "28       sentrysteve       188          True               False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to retrieve top few number of each category\n",
    "def get_top_data(top_n = 5000):\n",
    "    top_data_df_positive = top_data_df[top_data_df['haveBugs'] == 1].head(top_n)\n",
    "    top_data_df_negative = top_data_df[top_data_df['haveBugs'] == 0].head(top_n)\n",
    "    top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative])\n",
    "    return top_data_df_small\n",
    "\n",
    "# Function call to get the top 10000 from each review\n",
    "top_data_df_small = get_top_data(top_n=10000)\n",
    "\n",
    "# After selecting top few samples of each review\n",
    "print(\"After segregating and taking equal number of rows for each review:\")\n",
    "print(top_data_df_small['haveBugs'].value_counts())\n",
    "top_data_df_small.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess text data:\n",
    "1. Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant good service!!\n",
      "I like food!!\n",
      "This product good!!\n"
     ]
    }
   ],
   "source": [
    "# Removing the stop words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "print(remove_stopwords(\"Restaurant had a really good service!!\"))\n",
    "print(remove_stopwords(\"I did not like the food!!\"))\n",
    "print(remove_stopwords(\"This product is not good!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     [fun, game, ruined, by, overly, punishing, upd...\n",
      "5     [like, its, definitely, fun, but, the, stabili...\n",
      "6     [so, many, systems, to, learn, its, bit, overw...\n",
      "8     [this, game, needs, higher, level, cap, not, w...\n",
      "10    [the, game, has, film, grain, that, takes, awa...\n",
      "17    [the, game, itself, isnt, bad, its, unique, in...\n",
      "18    [have, to, put, this, game, down, because, all...\n",
      "20    [so, before, us, is, the, classic, browser, st...\n",
      "27    [this, game, is, cool, but, it, is, waaaaaay, ...\n",
      "28    [it, doesnt, make, sense, that, this, game, ha...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
    "print(top_data_df_small['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     [fun, game, ruin, by, overli, punish, updat, m...\n",
       "5     [like, it, definit, fun, but, the, stabil, is,...\n",
       "6     [so, mani, system, to, learn, it, bit, overwhe...\n",
       "8     [thi, game, need, higher, level, cap, not, wor...\n",
       "10    [the, game, ha, film, grain, that, take, awai,...\n",
       "17    [the, game, itself, isnt, bad, it, uniqu, in, ...\n",
       "18    [have, to, put, thi, game, down, becaus, all, ...\n",
       "20    [so, befor, us, is, the, classic, browser, str...\n",
       "27    [thi, game, is, cool, but, it, is, waaaaaai, t...\n",
       "28    [it, doesnt, make, sens, that, thi, game, ha, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
    "top_data_df_small['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into Train and Test Sets:\n",
    "Train data would be used to train the model and test data is the data on which the model would predict the classes and it will be compared with original labels to check the accuracy or other model test metrics.\n",
    "Train data ( Subset of data for training ML Model) ~70%\n",
    "Test data (Subset of data for testing ML Model trained from the train data) ~30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train reviews\n",
      "0    46\n",
      "1    28\n",
      "Name: haveBugs, dtype: int64\n",
      "Value counts for Test reviews\n",
      "0    14\n",
      "1    11\n",
      "Name: haveBugs, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index                                     stemmed_tokens\n",
      "0     65  [it, great, wai, to, portrai, semi, realist, p...\n",
      "1     92  [thi, is, realli, fun, game, with, plenti, to,...\n",
      "2     84  [return, it, interfac, imposs, to, negoti, eve...\n",
      "3     37  [if, you, ar, think, of, plai, thi, in, vr, yo...\n",
      "4     41  [rate, thi, posit, review, becaus, absolut, lo...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(top_data_df_small, test_size=0.25, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['stemmed_tokens']], \n",
    "                                                        top_data_df_small['haveBugs'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "\n",
    "    print(\"Value counts for Train reviews\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test reviews\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network for Text Classification:\n",
    "we are ready to dive into how will we use CNN for text classification and how the input will be constructed. CNN involves two operations, which can be thought of as feature extractors: convolution and pooling. Output of these operations is finally connected to the multi-layer perceptron to get the final output.\n",
    "Generating input and output tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(top_data_df_small))\n",
    "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "        temp_df = list(temp_df)\n",
    "        temp_df.append(['pad'])\n",
    "        word2vec_file = 'CNN/' + 'models/' + 'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = top_data_df_small['stemmed_tokens']\n",
    "        word2vec_file = 'CNN/' + 'models/' + 'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(top_data_df_small, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is ready, we can create a function to generate input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = top_data_df_small.stemmed_tokens.map(len).max()\n",
    "padding_idx = w2vmodel.wv.key_to_index[\"pad\"]\n",
    "\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1:\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0:\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "import gensim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2vmodel = gensim.models.KeyedVectors.load('CNN/' + 'models/' + 'word2vec_500_PAD.model')\n",
    "        weights = w2vmodel.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.key_to_index['pad'])\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # [B, T, E]\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alpha\\Downloads\\Thesis 2\\PreProcessing\\CNN.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/CNN.ipynb#ch0000022?line=34'>35</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/CNN.ipynb#ch0000022?line=36'>37</a>\u001b[0m \u001b[39m# Getting gradients w.r.t. parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/CNN.ipynb#ch0000022?line=37'>38</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/CNN.ipynb#ch0000022?line=39'>40</a>\u001b[0m \u001b[39m# Updating parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/CNN.ipynb#ch0000022?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2vmodel.wv)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Open the file for writing loss\n",
    "loss_file_name = 'CNN/' +  'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
    "f = open(loss_file_name,'w')\n",
    "f.write('iter, loss')\n",
    "f.write('\\n')\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in X_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = make_target(Y_train['haveBugs'][index])\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch ran :\"+ str(epoch+1))\n",
    "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
    "    f.write('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'CNN/' + 'cnn_big_model_500_with_padding.pth')\n",
    "\n",
    "f.close()\n",
    "print(\"Input vector\")\n",
    "print(bow_vec.cpu().numpy())\n",
    "print(\"Probs\")\n",
    "print(probs)\n",
    "print(torch.argmax(probs, dim=1).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iter', ' loss'], dtype='object')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      1.00      0.80        14\n",
      "           2       1.00      0.36      0.53        11\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.83      0.68      0.67        25\n",
      "weighted avg       0.81      0.72      0.68        25\n",
      "\n",
      "Index(['iter', ' loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAm00lEQVR4nO3deXhV5bn38e+dGRIIQxKGBAgEEBBQMaAgk6AWtZZKfVuxjkdLW6WttfZUX9tzrH09PW21atV6irMeW6rWWrUgIoM4QxCZBRJkSBgSCFMYEpLc7x97Y3dThkB2spO9f5/rysXaa6299/1c6+KXlWc961nm7oiISHSLi3QBIiLS+BT2IiIxQGEvIhIDFPYiIjFAYS8iEgMSIl1AXRkZGZ6bmxvpMkREWpTFixfvcPfMY21vdmGfm5tLQUFBpMsQEWlRzGzj8barG0dEJAYo7EVEYoDCXkQkBijsRURigMJeRCQGKOxFRGKAwl5EJAZETdjvOXCYh95ex7Li3ZEuRUSk2Wl2N1Wdqrg4eODttSQlxDE4p12kyxERaVai5sy+TUoindomU1RWEelSRESanagJe4C8zDSFvYjIUURf2JdWoEctioj8sygL+1T2HqpmR0VVpEsREWlWoivss9IA1JUjIlJHdIV9ZiDsC0sV9iIioaIq7Du3TaF1UrzO7EVE6oiqsI+LM3plplJUtj/SpYiINCtRFfbwjxE5IiLyD/UKezObYGZrzKzQzO44yvYeZjbHzJaZ2XwzywnZ9mszW2lmq83sd2Zm4WxAXXmZaZTsPsjBqprG/BoRkRblhGFvZvHAo8DFwABgspkNqLPbfcBz7j4YuAf4ZfC9I4DzgMHAQGAoMCZs1R/FkYu063fo7F5E5Ij6nNkPAwrdfb27VwHTgYl19hkAzA0uzwvZ7kAKkAQkA4nA9oYWfTx5WakA6rcXEQlRn7DPBjaHvC4Orgu1FJgUXL4caGNmHd39QwLhvzX4M8vdVzes5OPL7ZhKnKF+exGREOG6QHs7MMbMlhDopikBasysN9AfyCHwC2KcmY2q+2Yzm2JmBWZWUFZW1qBCUhLj6dahtYZfioiEqE/YlwDdQl7nBNd9wd23uPskdz8LuCu4bjeBs/yP3L3C3SuAmcDwul/g7tPcPd/d8zMzM0+tJSECE6KpG0dE5Ij6hP0ioI+Z9TSzJOBK4LXQHcwsw8yOfNadwFPB5U0EzvgTzCyRwFl/o3bjQGCOnPVlFdTWakI0ERGoR9i7ezUwFZhFIKhfdPeVZnaPmX0luNtYYI2ZrQU6AfcG178MFAHLCfTrL3X318PbhH+Vl5lGZXUtJbsPNvZXiYi0CPV6UpW7zwBm1Fn3HyHLLxMI9rrvqwG+3cAaT1rohGjdOrRu6q8XEWl2ou4OWvjHWHv124uIBERl2HdITaJ960SNyBERCYrKsIfA2b2mOhYRCYjqsF+vM3sRESCawz4rlR0VVew+oEcUiohEb9jrIq2IyBdiIOzVlSMiErVhn9O+FUnxcQp7ERGiOOwT4uPIzWhNUam6cUREojbsQSNyRESOiPqw31h+gKrq2kiXIiISUdEd9lmp1NQ6m8rVlSMisS26wz44IqdQ/fYiEuOiOux7afiliAgQ5WGflpxAl/QUhb2IxLyoDnvQIwpFRKCeYW9mE8xsjZkVmtkdR9new8zmmNkyM5tvZjkh27qb2VtmttrMVplZbhjrP6G8zFSKSitw1yMKRSR2nTDszSweeBS4GBgATDazAXV2uw94zt0HA/cAvwzZ9hzwG3fvDwwDSsNReH3lZaVRUVlN6b7KpvxaEZFmpT5n9sOAQndf7+5VwHRgYp19BgBzg8vzjmwP/lJIcPfZAO5e4e4HwlJ5PX0xR47mtheRGFafsM8GNoe8Lg6uC7UUmBRcvhxoY2Ydgb7AbjN7xcyWmNlvgn8p/BMzm2JmBWZWUFZWdvKtOA5NiCYiEr4LtLcDY8xsCTAGKAFqCDzQfFRw+1CgF3B93Te7+zR3z3f3/MzMzDCVFNCpbTKpSfG6SCsiMa0+YV8CdAt5nRNc9wV33+Luk9z9LOCu4LrdBP4K+DTYBVQNvAoMCUPd9WZm5GWl6cxeRGJafcJ+EdDHzHqaWRJwJfBa6A5mlmFmRz7rTuCpkPe2M7Mjp+vjgFUNL/vk5GWmqc9eRGLaCcM+eEY+FZgFrAZedPeVZnaPmX0luNtYYI2ZrQU6AfcG31tDoAtnjpktBwx4POytOIG8zFS27DnE/srqpv5qEZFmIaE+O7n7DGBGnXX/EbL8MvDyMd47GxjcgBob7MhF2s937GdgdnokSxERiYiov4MWAmPtQSNyRCR2xUTY9+jYmjjTWHsRiV0xEfbJCfF079Bawy9FJGbFRNjDkQnRdGYvIrEpdsI+K431O/ZTU6sJ0UQk9sRO2GemUlVdS8mug5EuRUSkycVQ2AcfUVi2L8KViIg0vZgL+yI9j1ZEYlDMhH371CQ6pibpIq2IxKSYCXvQiBwRiV2xFfZZqRprLyIxKbbCPjON8v1VlO+vinQpIiJNKubCHmC9unJEJMbEZNir315EYk1MhX12+1YkJcSp315EYk5MhX18nNErI1WzX4pIzKlX2JvZBDNbY2aFZnbHUbb3MLM5ZrbMzOabWU6d7W3NrNjMHglX4adKwy9FJBadMOzNLB54FLgYGABMNrMBdXa7D3jO3QcD9wC/rLP9F8CChpfbcHmZqWwqP0BldU2kSxERaTL1ObMfBhS6+3p3rwKmAxPr7DMAmBtcnhe63czOJvBc2rcaXm7D5WWlUeuwceeBSJciItJk6hP22cDmkNfFwXWhlgKTgsuXA23MrKOZxQH3E3jo+DGZ2RQzKzCzgrKysvpVfor+MUeOunJEJHaE6wLt7cAYM1sCjAFKgBrgZmCGuxcf783uPs3d8909PzMzM0wlHV3PjFRAwy9FJLYk1GOfEqBbyOuc4LovuPsWgmf2ZpYGfM3dd5vZcGCUmd0MpAFJZlbh7v9ykbeppCYn0DU9hUKd2YtIDKlP2C8C+phZTwIhfyVwVegOZpYBlLt7LXAn8BSAu38zZJ/rgfxIBv0ReVlpFOrMXkRiyAm7cdy9GpgKzAJWAy+6+0ozu8fMvhLcbSywxszWErgYe28j1RsWZ3Zrx4qSvbzyyXF7l0REooa5N69nsubn53tBQUGjfsehwzXc8PQiFm4o5/Frz2Zcv06N+n0iIo3NzBa7e/6xtsfUHbRHpCTGM+3as+nfpQ03v/AJBRvKI12SiEijismwB2iTksgzNwyja3or/u2ZRXy2bW+kSxIRaTQxG/YAGWnJPHfjMFolxXPtkwvZXK4brUQkOsV02APktG/N8zeeQ2V1LVc/+TFl+yojXZKISNjFfNgD9O3UhqdvGErp3kqufWohew8djnRJIiJhpbAPGtK9PY9dPYR12/dx07MFHDqsidJEJHoo7EOMPS2L+79+Bos2lDP1j0uorqmNdEkiImGhsK9j4pnZ3H3Z6by9ejt3vrKc5nYfgojIqajPdAkx57oRuezcX8Xv5qyjQ2oSd17SP9IliYg0iML+GH54QR927a/iDwvWM33RZjq3TaFTegpdgv92bptC5/RkOrUNLHdITcLMIl22iMhRKeyPwcz4+VdO57TObVizbR/b9h5i+95DrN66lx0VldTt3UlKiCO7XSvyMlPJy0wL/GSl0juzDemtEyPTCBGRIIX9ccTFGVef2+Nf1h+uqaVsX2XgF8CeQ2zdE/hFsHnXAYpK97Ng7Q6qQi7uZqQl0evIL4DMVHpnpXFGTjvapyY1ZXNEJIYp7E9BYnwcXdu1omu7VkfdXlPrFO86QFFZBYWlFRSV7qeorIKZK7ay+8A/xvD3yUpjaM8ODM1tz9DcDuS0b91UTRCRGBOTs15GUvn+KtZs28cnm3axaEM5izfsYl9lNQBd01OC4R/46ZOVRlycrgOIyImdaNZLndk3sQ6pSQzP68jwvI5A4K+Az7btZdHn5SzauIsPi3byt0+3ANCudSLDcjswvn8W55+WRVbblEiWLiItWL3O7M1sAvAQEA884e7/XWd7DwJPp8oEyoGr3b3YzM4EHgPaEngm7b3u/ufjfVe0n9mfiLuzqfwACz8vZ9GGct5bt4Mtew4BMDgnnXH9shjfrxOnd2170mf95furWLd9H62TEhiUk94Y5YtIhJzozP6EYW9m8cBa4EKgmMBjCie7+6qQfV4C3nD3Z81sHHCDu19jZn0Bd/d1ZtYVWAz0d/fdx/q+WA/7utydz7btY+5npcxZvZ0lm3fjDlltkhnXL4tx/bIY2SeD1kkJX+y/dc8hCksD1wvWlVZQVFpBYVkF5furvvjcMX0z+fGXTmNgtkJfJBqEI+yHA3e7+5eCr+8EcPdfhuyzEpjg7pstMNh8j7u3PcpnLQWucPd1x/o+hf3x7ayoZP6aMuZ+VsqCtWXsq6wmKSGO/B7t2V9ZTWFpBfur/jGvT7vWifTOTKNPp8BooN5ZaazZto/H3ili94HDXDKoM7ddeBq9s9Ii2CoRaahwhP0VBIL8puDra4Bz3H1qyD5/BD5294fMbBLwFyDD3XeG7DMMeBY4Pfhg8tDvmAJMAejevfvZGzduPMlmxqaq6loKNpQz57NSPizaSYfUJHpnpZGXlUafrECwdzzGzV57Dx3miXc/58l313PwcA1fG5LDDy7ooxFBIi1UU4V9V+ARoCewAPgaMPBId42ZdQHmA9e5+0fH+z6d2TetnRWV/H5+Ec9/tBEcrjqnO7ec35vMNsmRLk1ETkI4nkFbAnQLeZ0TXPcFd9/i7pPc/SzgruC63cEC2gJ/B+46UdBL0+uYlszPvjyA+beP5WtnZ/P8RxsZ/et5/GbWZ+w5oHn9RaJFfc7sEwhcoB1PIOQXAVe5+8qQfTKAcnevNbN7gRp3/w8zSwJmAq+7+4P1KUhn9pG1vqyCB95ex+tLt9A2JYGLTu/M6L6ZjOqdoTt+RZqxBnfjBD/kEuBBAkMvn3L3e83sHqDA3V8LdvX8EnAC3Ti3uHulmV0NPA2sDPm4693902N9l8K+eVi5ZQ9/eGc976wtY8/Bw5jB4Ox0RvfNZHTfTM7s1o7EeM2QLdJchCXsm5LCvnmpqXWWFu9mwdoyFqwt49PNu6l1aJOcwIjeHQPh3yeTbh10YVckkhT2ElZ7Dh7mg8IdLFhXxoK1OyjZfRCA/l3acvPYPC4Z1IV4TfEg0uQU9tJo3J2isv0sWFvGHxduorC0gl6ZqdwytjcTz+xKgrp5RJqMwl6aRG2t8+bKbTw8t5DVW/fSrUMrbh7bm0lDsklOiI90eSJRT2EvTcrdmbO6lIfnrmNp8R66pKfwnTF5fGNoN1ISFfoijUVhLxHh7ry7bgcPz13Hog27yGyTzJRRvbjqnO6kJmuyVZFwU9hLxH20fiePzC3kvcIdtG+dyJTReVw7vIdCXySMFPbSbCzeuIuH565j/poyOqYm8e0xvbjm3FxaJal7R6ShFPbS7HyyaRcPzF7Lu+t2kJGWzHfH5vHNc7qrT1+kART20mwt2lDOA7PX8kHRTrLaJHPL+b11IVfkFCnspdn7aP1Ofjt7LQs/L6dLego3n9+br+fnaMimyElQ2EuL4O58WLST+2evZfHGXWS3a8X3x/fmirO76Y5ckXoIxxTHIo3OzBjRO4OXvzOc5/5tGJltkvnJX5Zz2cPv8dH6nSf+ABE5LoW9NCtmxui+mfz15hE8PPks9hw8zJXTPuLmFxazufxApMsTabEU9tIsmRmXndGVOT8aw20X9mXeZ2WM/+07/PrNz6iorI50eSItjsJemrWUxHi+P74Pc28fw6WDuvD7+UWMu28+Ly8upra2eV1vEmnOFPbSInRJb8UD3ziTV24eQdd2rbj9paVc/vv3WbyxPNKlibQI9Qp7M5tgZmvMrNDM7jjK9h5mNsfMlpnZfDPLCdl2nZmtC/5cF87iJfYM6d6eV747gge+cQbb91bytcc+5Pt/WsL2vYciXZpIs1afZ9DGE3gG7YVAMYFn0E5291Uh+7wEvOHuz5rZOOAGd7/GzDoABUA+gUcWLgbOdvddx/o+Db2U+jpQVc3/zC/iDwvWkxQfxx2X9GPy0O7EaaimxKBwDL0cBhS6+3p3rwKmAxPr7DMAmBtcnhey/UvAbHcvDwb8bGDCyTRA5FhaJyVw20WnMevW0QzKSeeuv67gymkfUVRWEenSRJqd+oR9NrA55HVxcF2opcCk4PLlQBsz61jP92JmU8yswMwKysrK6lu7CAC5Gam8cNM5/PqKwazZvo+LH3yXR+auo6q6NtKliTQb4bpAezswxsyWAGOAEqCmvm9292nunu/u+ZmZmWEqSWKJmfH1/G7Mvm00F57eifveWstlD7/Hkk3H7DEUiSn1CfsSoFvI65zgui+4+xZ3n+TuZwF3Bdftrs97RcIpq00Kj141hCeuzWfvocNMeuwDfv76SvZrbL7EuPqE/SKgj5n1NLMk4ErgtdAdzCzDzI581p3AU8HlWcBFZtbezNoDFwXXiTSqCwZ04q0fjuaac3vwzAcbuOiBBcxbUxrpskQi5oRh7+7VwFQCIb0aeNHdV5rZPWb2leBuY4E1ZrYW6ATcG3xvOfALAr8wFgH3BNeJNLo2KYncM3EgL317OK2S4rnh6UVM/eMnlOw+GOnSRJqcZr2UmFBZXcNj84t4bH4RADeN6sl3x/YmTY9GlCihWS9FgOSEeG69oC9zbx/LxQM78+i8Isb+Zh5/WriJGk27IDFAYS8xJbtdKx688ixeveU8cjumcucry7n0d++yYK2G/Ep0U9hLTDqzWzte+s5wHvvmEA5U1XDtUwu5/umFrNu+L9KliTQKhb3ELDPj4kFdmH3baO66pD+LN+5iwkPv8tNXl7OzojLS5YmElcJeYl5yQjzfGt2Ld358Plef050/LdzM2N/M58+LNtHcBjCInCqFvUhQh9Qkfj5xILNuHc3A7HR+8pflfPd/P2HX/qpIlybSYAp7kTp6Z6Xxwk3n8H8v6cecz7Yz4aEFvF+4I9JliTSIwl7kKOLijCmj8/jrzeeRlpzAN5/4mHv/vorK6npP+STSrCjsRY5jYHY6b3xvFFef253H3/2crz76gUbsSIuksBc5gVZJ8fy/rw7iiWvzKd17iC8//B7Pf7hBF2+lRVHYi9TTBQM6MfPWUZzbqyM/+9tKbny2gLJ9GqIpLYPCXuQkZLVJ4ZkbhnL3ZQN4r3AHFz+0gDmrt0e6LJETUtiLnCQz4/rzevL61JFkpCVz47MF/GD6EnboRixpxhT2IqfotM5t+NvU87j1gj7MWL6VC377Di8vLlZfvjRLCnuRBjgym+aM748iLzON219ayjVPLmTTzgORLk3knyjsRcKgT6c2vPTt4fxi4ul8unk3Fz34DtMWFFFdo4eeS/NQr7A3swlmtsbMCs3sjqNs725m88xsiZktM7NLgusTzexZM1tuZqvN7M5wN0CkuYiLM64Znsvs20YzsncG/zXjM776+/dZUbIn0qWJnDjszSweeBS4GBgATDazAXV2+ymBxxWeReAZtb8Prv8/QLK7DwLOBr5tZrlhql2kWeqS3orHr83n0auGsG1PJRMffZ9fzlzNwSrdfSuRU58z+2FAobuvd/cqYDowsc4+DrQNLqcDW0LWp5pZAtAKqAL2NrhqkWbOzLh0cBfm3DaGK4bk8Id31jPhoQW8t05z7Ehk1Cfss4HNIa+Lg+tC3Q1cbWbFwAzge8H1LwP7ga3AJuC+oz1w3MymmFmBmRWUlemJQRI90lsn8qsrBvOnb51LnBlXP/kxP/zzp5ovX5pcuC7QTgaecfcc4BLgeTOLI/BXQQ3QFegJ/MjMetV9s7tPc/d8d8/PzMwMU0kizcfwvI7M/MEovjeuN28s28L4377DiwWbNUxTmkx9wr4E6BbyOie4LtSNwIsA7v4hkAJkAFcBb7r7YXcvBd4Hjvn0c5FolpIYz48uOo0Z3x9F78w0/v3lZUx+/CPWl1VEujSJAfUJ+0VAHzPraWZJBC7AvlZnn03AeAAz608g7MuC68cF16cC5wKfhad0kZapT6c2vPjt4fzX5YNYuWUvEx58l4feXqfpk6VRnTDs3b0amArMAlYTGHWz0szuMbOvBHf7EfAtM1sK/Am43gN/nz4KpJnZSgK/NJ5292WN0RCRliQuzrjqnO7MuW0MF57eiQfeXsulv3uPhZ//yyUtkbCw5tZnmJ+f7wUFBZEuQ6RJzfuslJ++uoKS3QeZPKwbd0zoT3rrxEiXJS2ImS1292N2k+sOWpFm4Px+Wcy+bTTfGtWTFwuKGf/b+by6pEQXcCVsFPYizUTrpATuunQAf7vlPLLbt+bWP3/K1U9+rAu4EhYKe5FmZmB2Oq98dwS/+OpAlhXvYcKD7/LA7LUcOqwLuHLqFPYizVB8nHHNuT2Y86MxTBjYmYfmrOPih97VHbhyyhT2Is1YVpsUfjf5LJ6/cRjuztVPfswPpi+hdN+hSJcmLYzCXqQFGNUnkzdvHc33x/dh5vJtjL//HZ7/aCO1tbqAK/WjsBdpIVIS47ntwr7MvHUUg7LT+dmrK5j02Ad8smlXpEuTFkBhL9LC5GWm8cJN5/DAN86geNcBJv3+A77z/GKKNGpHjiMh0gWIyMkzMy4/K4eLBnTmiXc/Z9qCImav3s43hnbj1vF9yGqbEukSpZnRHbQiUWBHRSUPz1nHCx9vIjE+jhtH9mTKmF60TdFduLHiRHfQKuxFosjGnfu57621vL50C+1bJzJ1XB+uPrc7yQnxkS5NGpmmSxCJIT06pvLw5LN4fepIBnRtyy/eWMX4+9/h1SUlGrkT4xT2IlFoUE46L9x0Ls/fOIz0Vonc+udPufTh95i3plTz7cQohb1IFBvVJ5PXp47koSvPZH9lNTc8vYgrp32k4ZoxSGEvEuXi4oyJZ2bz9m1juGfi6RSVVTDp9x8w5bkCCkv3Rbo8aSL1Cnszm2Bma8ys0MzuOMr27mY2z8yWmNkyM7skZNtgM/vQzFaa2XIz05gwkQhISojj2uG5vPPj87ntwr58ULSTix5YwI9fWsqW3QcjXZ40shOOxjGzeGAtcCFQTOCJU5PdfVXIPtOAJe7+mJkNAGa4e66ZJQCfANe4+1Iz6wjsdvdjTt+n0TgiTaN8fxWPzivk+Q83gsF1w3tw89jetE9NinRpcgrCMRpnGFDo7uvdvQqYDkyss48DbYPL6cCW4PJFwDJ3Xwrg7juPF/Qi0nQ6pCbxsy8PYO7tY7hscFeeeO9zRv96Ho/MXcf+yupIlydhVp+wzwY2h7wuDq4LdTdwtZkVAzOA7wXX9wXczGaZ2Sdm9u9H+wIzm2JmBWZWUFZWdlINEJGGyWnfmvu/fgZv/mA05/TqyH1vrWXkr+by6LxC9h06HOnyJEzCdYF2MvCMu+cAlwDPm1kcgekYRgLfDP57uZmNr/tmd5/m7vnunp+ZmRmmkkTkZJzWuQ1PXJfPKzeP4Mxu7fjNrDWM/NU8fjdnHXsOKvRbuvqEfQnQLeR1TnBdqBuBFwHc/UMgBcgg8FfAAnff4e4HCJz1D2lo0SLSeIZ0b8/TNwzjtannMTS3Pb+dHTjTf2D2WvYcUOi3VPUJ+0VAHzPraWZJwJXAa3X22QSMBzCz/gTCvgyYBQwys9bBi7VjgFWISLM3OKcdT1w3lDe+N5IReR15aM46zvvVXO6btYZd+6siXZ6cpHrNjRMcSvkgEA885e73mtk9QIG7vxYcgfM4kEbgYu2/u/tbwfdeDdwZXD/D3Y/ab3+ERuOINE+rtuzlkXnrmLF8G6lJ8Vw7IpebRvakY1pypEsTNBGaiITZmm37eGReIW8s20JSfByThuRw48hceme1iXRpMU1hLyKNorC0giffW89fPimhqrqW80/L5MaRvTivd0fMLNLlxRyFvYg0qp0Vlbzw8Sae+3ADOyqq6Ne5Df82sicTz+yqqZWbkMJeRJrEocM1vLZ0C0+99zmfbdtHRloy1w7vwTfP6a5+/SagsBeRJuXuvF+4kyfeW8/8NWUkJ8QxaUg2N5zXk76d1K/fWE4U9noGrYiElZkxsk8GI/tkUFi6jyff28ArnxTzp4WbGZHXketH5DK+fyfi49Sv35R0Zi8ijW7X/iqmL9rM8x9uYMueQ+S0b8V1w3P5en430lvrObnhoG4cEWk2qmtqmb1qO09/sIGFn5fTKjGey4dkc/2IXHXxNJDCXkSapZVb9vDsBxv426dbqKyu5bzeHbl+RE/G9ctSF88pUNiLSLNWvr+K6Ys28fyHG9m65xDdOgS7eIZ2o22KunjqS2EvIi1CdU0ts1Zu55kPPmfRhl20TornirNzuH5ELr0y0yJdXrOnsBeRFmd58R6e/uBz3li6laqawN25N5zXk1F9MnR37jEo7EWkxSrbV8kLH2/kfz/axI6KSnpnpXH9iFwmDcmmdZJGjodS2ItIi1dZXcPfl23l6fc3sLxkD21TEpg8rDvfGNpNXTxBCnsRiRruzuKNu3j6/Q28uXIbNbVOfo/2XHF2DpcO7kKbGL6gq7AXkai0fe8h/rqkhJcKNlNUtp+UxDguHtiFK87OYXivjsTF2PBNhb2IRDV359PNu3l5cTGvLd3CvkPVZLdrxaQh2Vxxdg49OqZGusQmEZawN7MJwEMEnlT1hLv/d53t3YFngXbBfe5w9xl1tq8C7nb3+473XQp7ETlVhw7X8Naq7by8uJh315XhDsNyOzBpSDYTBnamXeukSJfYaBoc9mYWD6wFLiTwAPFFwGR3XxWyzzRgibs/FnxE4Qx3zw3Z/jKBxxJ+rLAXkaawdc9BXvmkhL8sLmb9jv0kxAUmaPvy4K5cdHqnqLthKxyzXg4DCt19ffADpwMT+ecHhzvQNricDmwJKeCrwOfA/pOqXESkAbqkt+KW83tz89g8VpTs5Y1lW3hj2VZuf2kpSa/EMbpvJped0YXx/TuRlhz9wzjr08JsYHPI62LgnDr73A28ZWbfA1KBCwDMLA34CYG/Cm4/1heY2RRgCkD37t3rWbqIyImZGYNy0hmUk84dF/djyebdvLF0K39fvoW3V28nOSGOcf2y+PLgrozrl0WrpOh8ula4fp1NBp5x9/vNbDjwvJkNJPBL4AF3rzjeXW/uPg2YBoFunDDVJCLyT8yMId3bM6R7e356aX8KNu7ijWVbmLF8GzNXbKNVYjzj+mdxycAunN8vM6pu3KpPS0qAbiGvc4LrQt0ITABw9w/NLAXIIPAXwBVm9msCF29rzeyQuz/S0MJFRBoiLs4Y1rMDw3p24D8vO52P1+/kjeVbmbViG39ftpWUxDjG9s3iksFdGNcvq8V39dTnAm0CgQu04wmE/CLgKndfGbLPTODP7v6MmfUH5gDZHvLhZnY3UKELtCLSnNXUOgs/L2fmiq3MXLGNsn2VJCXEMaZvJpcM6sz4/s3z4m6DL9C6e7WZTQVmERhW+ZS7rzSze4ACd38N+BHwuJn9kMDF2uu9PmM6RUSamfg4Y3heR4bndeQ/LzudxRt3MWP5Vt5csY3Zq7aTFB/HqD4ZfGlgZ8b3y2oxD1PXTVUiIvVQW+ss2bybmcsDZ/wluw8SZ5DfowMXDujEhQM6kZsRuRu4dAetiEiYuTsrSvYye9U23lq1nc+27QOgT1baF8F/Rk67Jp2yQWEvItLINpcfYPaq7cxetZ2FG8qpqXWy2iQzvn8nLjq9EyPyOpKc0LhDOhX2IiJNaPeBKuatKWX2qu28s6aM/VU1tElOYHz/LCYM7MKYvpmNMpZfYS8iEiGHDtfwYdFOZq7YyuxV29l14DCtEuM5v18mEwZ24fzTMsM2LXM4pksQEZFTkJIYz/n9sji/XxbVNbV8HBzSOWvldmYs30ZSQhyj+2QwYWAXLuzfifTWjTekU2f2IiJNrKbW+WTTLmYu38abK7ayZc8hEuKMCQM788hVQ07pM3VmLyLSzMTHGUNzOzA0twM/+3J/lhXvYeaKbcTHNd53KuxFRCLIzDijWzvO6NauUb+nEX+PiIhIc6GwFxGJAQp7EZEYoLAXEYkBCnsRkRigsBcRiQEKexGRGKCwFxGJAc1uugQzKwM2NuAjMoAdYSqnOYi29kD0tSna2gPR16Zoaw/8a5t6uHvmsXZudmHfUGZWcLz5IVqaaGsPRF+boq09EH1tirb2wMm3Sd04IiIxQGEvIhIDojHsp0W6gDCLtvZA9LUp2toD0demaGsPnGSboq7PXkRE/lU0ntmLiEgdCnsRkRgQNWFvZhPMbI2ZFZrZHZGuJxzMbIOZLTezT82sxT2r0cyeMrNSM1sRsq6Dmc02s3XBf9tHssaTdYw23W1mJcHj9KmZXRLJGk+GmXUzs3lmtsrMVprZD4LrW+RxOk57WvIxSjGzhWa2NNimnwfX9zSzj4OZ92czSzru50RDn72ZxQNrgQuBYmARMNndV0W0sAYysw1Avru3yJtBzGw0UAE85+4Dg+t+DZS7+38Hfym3d/efRLLOk3GMNt0NVLj7fZGs7VSYWRegi7t/YmZtgMXAV4HraYHH6Tjt+Tot9xgZkOruFWaWCLwH/AC4DXjF3aeb2f8AS939sWN9TrSc2Q8DCt19vbtXAdOBiRGuKea5+wKgvM7qicCzweVnCfxHbDGO0aYWy923uvsnweV9wGogmxZ6nI7TnhbLAyqCLxODPw6MA14Orj/hMYqWsM8GNoe8LqaFH+AgB94ys8VmNiXSxYRJJ3ffGlzeBnSKZDFhNNXMlgW7eVpEl0ddZpYLnAV8TBQcpzrtgRZ8jMws3sw+BUqB2UARsNvdq4O7nDDzoiXso9VIdx8CXAzcEuxCiBoe6ENs+f2I8BiQB5wJbAXuj2g1p8DM0oC/ALe6+97QbS3xOB2lPS36GLl7jbufCeQQ6Mnod7KfES1hXwJ0C3mdE1zXorl7SfDfUuCvBA5yS7c92K96pH+1NML1NJi7bw/+Z6wFHqeFHadgP/BfgBfc/ZXg6hZ7nI7WnpZ+jI5w993APGA40M7MEoKbTph50RL2i4A+wavTScCVwGsRrqlBzCw1eIEJM0sFLgJWHP9dLcJrwHXB5euAv0WwlrA4EopBl9OCjlPw4t+TwGp3/23IphZ5nI7VnhZ+jDLNrF1wuRWBgSirCYT+FcHdTniMomI0DkBwKNWDQDzwlLvfG9mKGsbMehE4mwdIAP7Y0tpkZn8CxhKYinU78J/Aq8CLQHcCU1l/3d1bzAXPY7RpLIHuAQc2AN8O6e9u1sxsJPAusByoDa7+vwT6uVvccTpOeybTco/RYAIXYOMJnKC/6O73BDNiOtABWAJc7e6Vx/ycaAl7ERE5tmjpxhERkeNQ2IuIxACFvYhIDFDYi4jEAIW9iEgMUNiLiMQAhb2ISAz4//bYX0pUOZfZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "bow_cnn_predictions = []\n",
    "original_lables_cnn_bow = []\n",
    "cnn_model.eval()\n",
    "loss_df = pd.read_csv('CNN/' +  'plots/' +'cnn_class_big_loss_with_padding.csv')\n",
    "print(loss_df.columns)\n",
    "# loss_df.plot('loss')\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "        original_lables_cnn_bow.append(make_target(Y_test['haveBugs'][index]).cpu().numpy()[0])\n",
    "print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
    "loss_file_name = 'CNN/' +  'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
    "loss_df = pd.read_csv(loss_file_name)\n",
    "print(loss_df.columns)\n",
    "plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
    "fig = plt_500_padding_30_epochs.get_figure()\n",
    "fig.savefig('CNN/' +'plots/' + 'loss_plt_500_padding_30_epochs.pdf')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee4488239c6fa38dff4966fe16af52ffb7f6a3e81a87d3252888b0f8931c9e83"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
