{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Transformers\n",
    "Installing the Transformers library is fairly easy. Just run the following pip line on a Google Colab cell:\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained BERT Tokenizer and Sequence Classifier as well as InputExample and InputFeatures.Then, we will build our model with the Sequence Classifier and our tokenizer with BERT’s Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alpha\\Downloads\\Thesis 2\\PreProcessing\\Bert Sentimental Analysis.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/Bert%20Sentimental%20Analysis.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, TFBertForSequenceClassification\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/Bert%20Sentimental%20Analysis.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m InputExample, InputFeatures\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/Bert%20Sentimental%20Analysis.ipynb#ch0000002?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m TFBertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alpha/Downloads/Thesis%202/PreProcessing/Bert%20Sentimental%20Analysis.ipynb#ch0000002?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\file_utils.py:860\u001b[0m, in \u001b[0;36mDummyObject.__getattr__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=857'>858</a>\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=858'>859</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39mcls\u001b[39m, key)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=859'>860</a>\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\file_utils.py:848\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=845'>846</a>\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m    <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=846'>847</a>\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alpha/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/file_utils.py?line=847'>848</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nTFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of our BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Pandas to View and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Dataset/cleanedReviewsDateset.csv\",low_memory=False,encoding=\"ISO-8859-1\")\n",
    "train_set = dataset[0:60000]\n",
    "valid_set = dataset[60000:65000]\n",
    "test_set  = dataset[65000:77762]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create labels using the Vader algorithm. It is an unsupervised, rule-based method for sentiment analysis, and it is accessible through the NLTK package. You need to install the NLTK library (pip install nltk), then download the Vader package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict each review’s sentiment using the Vader algorithm with the following code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    \n",
    "    if scores[\"neg\"] > scores[\"pos\"]:\n",
    "        return 0\n",
    "\n",
    "    return 1\n",
    "\n",
    "train_set[\"vader_result\"] = train_set[\"text\"].apply(lambda x: vader_sentiment_result(x))\n",
    "valid_set[\"vader_result\"] = valid_set[\"text\"].apply(lambda x: vader_sentiment_result(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Huggingface Model with the Trainer function:\n",
    "The process starts with converting the data to a PyTorch dataset object to feed it to the BERT model. It is a class for preprocessing and presenting the data. Loading the BERT classifier from the Huggingface library and fine-tune the model with the Trainer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load the BERT Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# The dataset class\n",
    "class TheDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, reviews, sentiments, tokenizer):\n",
    "        self.reviews    = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = tokenizer.model_max_length\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        review = str(self.reviews[index])\n",
    "        sentiments = self.sentiments[index]\n",
    "\n",
    "        encoded_review = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens    = True,\n",
    "            max_length            = self.max_len,\n",
    "            return_token_type_ids = False,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors        = 'pt',\n",
    "            padding               = \"max_length\",\n",
    "            truncation            = True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_review['input_ids'][0],\n",
    "            'attention_mask': encoded_review['attention_mask'][0],\n",
    "            'labels': torch.tensor(sentiments, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare the Train/Validation sets\n",
    "train_set_dataset = TheDataset(\n",
    "    reviews    = train_set.text.tolist(),\n",
    "    sentiments = train_set.vader_result.tolist(),\n",
    "    tokenizer  = tokenizer,\n",
    ")\n",
    "\n",
    "valid_set_dataset = TheDataset(\n",
    "    reviews    = valid_set.text.tolist(),\n",
    "    sentiments = valid_set.vader_result.tolist(),\n",
    "    tokenizer  = tokenizer,\n",
    ")\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Freeze BERT except (the 24th layer + the last pooler layer)\n",
    "for name, param in model.bert.named_parameters():\n",
    "    if ( not name.startswith('pooler') ) and \"layer.23\" not in name :\n",
    "        param.requires_grad = False\n",
    "\n",
    "# The function to get the accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define the training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = \"./sentiment-analysis\",\n",
    "    num_train_epochs            = 10,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 64,\n",
    "    warmup_steps                = 500,\n",
    "    weight_decay                = 0.01,\n",
    "    save_strategy               = \"epoch\",\n",
    "    evaluation_strategy         = \"steps\"\n",
    ")\n",
    "\n",
    "# Define the Huggingface Trainer object\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_set_dataset,\n",
    "    eval_dataset    = valid_set_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "# Start pre-training!\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04c454af8dcba4ffd51d9f2a111662a74e12e3c4adee75c16ec6fee7890455df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
